[build-system]
requires = ["scikit-build-core>=0.9", "pybind11>=2.11"]
build-backend = "scikit_build_core.build"

[tool.scikit-build]
wheel.packages = ["lazyllm"]
cmake.source-dir = "csrc"
cmake.build-type = "Release"
build.verbose = true

[tool.cibuildwheel]
# Skip musllinux (Alpine) and all 32bits systems.
skip = ["*-musllinux_*", "*i686", "*-win32"]
build = "cp310-* cp311-* cp312-*"
manylinux-x86_64-image = "manylinux2014"

[project]
name = "lazyllm"
version = "0.7.0"
description = "A Low-code Development Tool For Building Multi-agent LLMs Applications."
authors = [{name = "wangzhihong", email = "wangzhihong@sensetime.com"}]
license = "Apache-2.0"
readme = "README.md"
requires-python = ">=3.10,<3.13"
dependencies = [
    "toml",
    "fastapi >=0.111.0",
    "loguru >=0.7.2",
    "pydantic >=2.11.7,<3.0.0",
    "requests >=2.32.2",
    "uvicorn >=0.23.2,<0.24.0",
    "cloudpickle >=3.0.0",
    "gradio ==5.49.1",
    "gradio-client >=0.6.1",
    "protobuf >=3.20.1",
    "docstring-parser >=0.16,<0.17",
    "json5 >=0.9.25,<0.10.0",
    "tiktoken >=0.7.0,<0.8.0",
    "spacy <=3.7.5",
    "bm25s >=0.1.10,<0.2.0",
    "pystemmer >=2.2.0.1,<3.0.0",
    "nltk >=3.8.1,<4.0.0",
    "jieba >=0.42.1",
    "pyjwt >=2.8.0",
    "sentencepiece >=0.2.0,<0.3.0",
    "psycopg2-binary >=2.9.9,<3.0.0",
    "sqlalchemy >=2.0.34,<3.0.0",
    "psutil >=6.0.0,<7.0.0",
    "pypdf >=5.0.0,<6.0.0",
    "numpy ==1.26.4",
    "async-timeout >=5.0.1,<6.0.0",
    "httpx <0.28.0",
    "docx2txt >=0.9,<0.10",
    "ebooklib >=0.19,<0.20",
    "html2text >=2025.4.15,<2026.0.0",
    "olefile >=0.47,<0.48",
    "openpyxl >=3.1.5,<4.0.0",
    "python-pptx >=1.0.2,<2.0.0",
    "tenacity >=9.1.2,<10.0.0",
    "beautifulsoup4 >=4.13.4,<5.0.0",
    "deepdiff >=8.6.1",
]

[project.scripts]
lazyllm = "lazyllm.cli.main:main"

[project.optional-dependencies]
appdirs = ["appdirs"]
loralib = ["loralib"]
flake8 = ["flake8>=7.0.0"]
chromadb = ["chromadb>=1.0.6"]
sentence-transformers = ["sentence-transformers>=3.0.1,<4.0.0"]
modelscope = ["modelscope==1.27.1"]
pytest = ["pytest>=8.3.3,<9.0.0"]
pymilvus = ["pymilvus>=2.4.11,<2.5.0"]
rapidfuzz = ["rapidfuzz>=3.12.2,<4.0.0"]
redis = ["redis>=5.0.4"]
huggingface-hub = ["huggingface-hub>=0.23.1"]
pandas = ["pandas>=2.2.2"]
rank-bm25 = ["rank-bm25>=0.2.2"]
redisvl = ["redisvl>=0.1.3"]
datasets = ["datasets>=2.18.0"]
deepspeed = ["deepspeed>=0.12.3"]
fire = ["fire>=0.6.0"]
peft = ["peft==0.17.1"]
torch = ["torch>=2.1.2"]
transformers = ["transformers==4.57.1"]
collie-lm = ["collie-lm>=1.0.7"]
faiss-cpu = ["faiss-cpu>=1.8.0"]
google = ["google>=3.0.0"]
scikit-learn = ["scikit-learn>=1.5.0"]
tensorboard = ["tensorboard>=2.16.2"]
tensorboard-data-server = ["tensorboard-data-server>=0.7.2"]
torchvision = ["torchvision>=0.16.2"]
wandb = ["wandb>=0.17.0"]
chattts = ["chattts>=0.2.4,<0.3.0"]
funasr = ["funasr>=1.1.4,<2.0.0"]
timm = ["timm>=1.0.8,<2.0.0"]
diffusers = ["diffusers==0.35.2"]
sortedcontainers = ["sortedcontainers>=2.4.0,<3.0.0"]
flash-attn = ["flash-attn==2.8.0.post2"]
lazyllm-llamafactory = ["lazyllm-llamafactory==0.9.4.dev2"]
rotary-embedding-torch = ["rotary-embedding-torch>=0.8.3,<0.9.0"]
infinity-emb = ["infinity-emb==0.0.77"]
ctranslate2 = ["ctranslate2>=4.0.0,<5.0.0"]
optimum = ["optimum==1.24.0"]
typer = ["typer>=0.12.5,<0.13.0"]
pymongo = ["pymongo>=4.12.1,<5.0.0"]
pymysql = ["pymysql>=1.1.1,<2.0.0"]
flagembedding = ["flagembedding==1.3.5"]
mcp = ["mcp>=1.5.0"]
pytesseract = ["pytesseract>=0.3.13,<0.4.0"]
openai-whisper = ["openai-whisper"]
qwen-vl-utils = ["qwen-vl-utils>=0.0.11,<0.0.12"]
accelerate = ["accelerate==1.6.0"]
lazyllm-lmdeploy = ["lazyllm-lmdeploy==0.10.2.dev0"]
boto3 = ["boto3>=1.39.3,<2.0.0"]
botocore = ["botocore>=1.39.3,<2.0.0"]
ftfy = ["ftfy==6.3.1"]
imageio = ["imageio==2.37.0"]
imageio-ffmpeg = ["imageio-ffmpeg==0.6.0"]
volcengine-python-sdk = ["volcengine-python-sdk>=4.0.6"]
dashscope = ["dashscope>=1.23.6"]
zhipuai = ["zhipuai>=2.1.5.20250708"]
opensearch-py = ["opensearch-py>=3.0.0"]
elasticsearch = ["elasticsearch<7.17.12"]
tokenizers = ["tokenizers==0.22.0"]
lazyllm-verl = ["lazyllm-verl==0.3.2.dev2"]
bitsandbytes = ["bitsandbytes==0.48.2"]
pluggy = ["pluggy==1.6.0"]
chardet = ["chardet>=5.2.0,<6.0.0"]

dev = [
    "flake8",
    "pytest",
]
standard = [
    "dev",
    "appdirs",
    "chromadb",
    "loralib",
    "modelscop",
    "pymilvus",
    "rapidfuzz",
    "sentence-transformers",
    "datasets",
    "deepspeed",
    "faiss-cpu",
    "fire",
    "google",
    "pandas",
    "peft",
    "rank-bm25",
    "scikit-learn",
    "torch",
    "torchvision",
    "transformers",
    "vllm",
    "wandb",
    "chattts",
    "funasr",
    "lazyllm-lmdeploy",
    "rotary-embedding-torch",
    "infinity-emb",
    "ctranslate2",
    "optimum",
    "typer",
    "flagembedding",
    "pytesseract",
    "diffusers",
    "ftfy",
    "imageio",
    "imageio-ffmpeg",
    "lazyllm-verl",
    "bitsandbytes"
]
alpaca-lora = [
    "appdirs",
    "datasets",
    "deepspeed",
    "faiss-cpu",
    "fire",
    "loralib",
    "peft",
    "sentence-transformers",
    "tensorboard",
    "tensorboard-data-server",
    "torch",
    "transformers"
]
colie = [ 
    "collie-lm",
    "peft",
    "tensorboard",
    "tensorboard-data-server",
    "torch"
]
llama-factory = [
    "datasets",
    "deepspeed",
    "lazyllm-llamafactory",
    "peft",
    "tensorboard",
    "tensorboard-data-server",
    "torch",
    "transformers",
    "accelerate",
    "qwen-vl-utils",
    "lazyllm-lmdeploy"
]
finetune-all = [
    "appdirs",
    "collie-lm",
    "ctranslate2",
    "datasets",
    "deepspeed",
    "faiss-cpu",
    "fire",
    "flagembedding",
    "flash-attn",
    "huggingface-hub",
    "lazyllm-llamafactory",
    "loralib",
    "modelscope",
    "optimum",
    "pandas",
    "peft",
    "rotary-embedding-torch",
    "scikit-learn",
    "sentence-transformers",
    "tensorboard",
    "tensorboard-data-server",
    "timm",
    "torch",
    "torchvision",
    "transformers",
    "wandb" 
]
model_deps = [
    "huggingface-hub",
    "modelscope"
]
vllm = [ "model_deps", "vllm==0.10.1"]
lmdeploy = ["model_deps", "lazyllm-lmdeploy"]
lightllm = ["model_deps", "lightllm>=0.0.1,<0.0.2"]
infinity = ["model_deps", "infinity-emb"  ]
deploy-all = [
    "model_deps",
    "vllm==0.10.1",
    "sentence-transformers",
    "lazyllm-lmdeploy",
    "infinity-emb"  
]
multimodal = [
    "chattts",
    "diffusers",
    "funasr",
    "openai-whisper",
    "pytesseract",
    "torch",
    "transformers"
]
rag-advanced = [
    "chromadb",
    "ctranslate2",
    "datasets",
    "flagembedding",
    "huggingface-hub",
    "infinity-emb",
    "modelscope",
    "pandas",
    "pymilvus",
    "pymongo",
    "pymysql",
    "pytesseract",
    "rank-bm25",
    "rapidfuzz",
    "redis",
    "redisvl",
    "sentence-transformers",
    "torch",
    "transformers",
    "boto3",
    "botocore",
    "opensearch-py",
    "elasticsearch"
]
agent-advanced = [
    "ctranslate2",
    "mcp"
]
online-advanced = [
    "volcengine-python-sdk",
    "dashscope",
    "zhipuai"
]
full = [
    "standard",
    "alpaca-lora",
    "colie",
    "llama-factory",
    "finetune-all",
    "vllm",
    "lmdeploy",
    "lightllm",
    "infinity",
    "deploy-all",
    "multimodal",
    "rag-advanced",
    "agent-advanced",
    "online-advanced"
]

[tool.lazyllm.extras_descriptions]
standard = "Install minimal dependencies for all LazyLLM features. Supports online model fine-tuning and inference, as well as offline model fine-tuning (requires LLaMA-Factory) and inference (requires vLLM)"
full = "Install all dependencies for LazyLLM, enabling every feature and advanced tools: automatic framework selection (AutoFinetune, AutoDeploy), additional offline inference engines (LightLLM), and extra training tools (AlpacaloraFinetune, CollieFinetune)"
alpaca-lora = "Install dependencies for the Alpaca-LoRA fine-tuning framework for local model training"
colie = "Install dependencies for the Collie fine-tuning framework for local model training"
llama-factory = "Install dependencies for LLaMA-Factory fine-tuning framework"
finetune-all = "Install all fine-tuning frameworks, including alpaca-lora, colie, and llama-factory"
vllm = "Install dependencies for vLLM local model inference framework"
lmdeploy = "Install dependencies for LMDeploy local model inference framework"
lightllm = "Install dependencies for LightLLM local model inference framework"
infinity = "Install dependencies for local embedding inference using the Infinity framework"
deploy-all = "Install all local inference frameworks, including LightLLM, vLLM, LMDeploy, and Infinity"
multimodal = "Install dependencies for multimodal features (speech generation, text-to-image, etc.)"
rag-advanced = "Install advanced RAG features, including vector database support and embedding fine-tuning"
agent-advanced = "Install advanced agent-related features with MCP support"
dev = "Install developer dependencies for code style checks and testing scripts"
online-advanced = "Install dependencies for online multimodal"

[tool.pytest.ini_options]
markers = [
    "run_on_change: skip test unless related files changed",
    "ignore_cache_on_change: use cache unless related files changed",
    "skip_on_win: mark tests to skip on Windows",
    "skip_on_mac: mark tests to skip on macOS",
    "skip_on_linux: mark tests to skip on Linux",
]
order_group_scope = "class"
