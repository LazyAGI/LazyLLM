[build-system]
requires = ["scikit-build-core>=0.9", "pybind11>=2.11"]
build-backend = "scikit_build_core.build"

[tool.scikit-build]
wheel.packages = ["lazyllm"]
cmake.source-dir = "csrc"
cmake.build-type = "Release"
build.verbose = true

[tool.cibuildwheel]
# Skip musllinux (Alpine) and all 32bits systems.
skip = ["*-musllinux_*", "*i686", "*-win32"]
build = "cp310-* cp311-* cp312-*"
manylinux-x86_64-image = "manylinux2014"

[project]
name = "lazyllm"
version = "0.7.3"
description = "A Low-code Development Tool For Building Multi-agent LLMs Applications."
authors = [{name = "wangzhihong", email = "wangzhihong@sensetime.com"}]
license = "Apache-2.0"
readme = "README.md"
requires-python = ">=3.10,<3.13"
dependencies = [
    "toml",
    "tqdm",
    "filelock",
    "pyyaml",
    "packaging",
    "fastapi >=0.111.0",
    "loguru >=0.7.2",
    "pydantic >=2.11.7,<3.0.0",
    "requests >=2.32.2",
    "uvicorn >=0.23.2",
    "cloudpickle >=3.0.0",
    "docstring-parser >=0.16,<0.17", # tools/agent
    "json5 >=0.9.25,<0.10.0",
    "pyjwt >=2.8.0",
    "psutil >=6.0.0,<7.0.0",
    "deepdiff >=8.6.1",
]

[project.scripts]
lazyllm = "lazyllm.cli.main:main"

[project.optional-dependencies]
optional = [
    "accelerate==1.6.0",
    "appdirs",
    "async-timeout>=5.0.1,<6.0.0",
    "beautifulsoup4>=4.13.4,<5.0.0",
    "bitsandbytes==0.48.2",
    "bm25s>=0.1.10,<0.2.0",
    "boto3>=1.39.3,<2.39.3",
    "botocore>=1.39.3,<2.39.3",
    "chardet>=5.2.0,<6.2.0",
    "chattts>=0.2.4,<0.3.4",
    "chromadb>=1.0.6",
    "collie-lm>=1.0.7",
    "ctranslate2>=4.0.0,<5.0.0",
    "dashscope>=1.23.6",
    "datasets>=2.18.0",
    "deepspeed>=0.12.3",
    "diffusers==0.35.2",
    "docx2txt>=0.9,<0.10",
    "ebooklib>=0.19,<0.20",
    "elasticsearch<7.17.12",
    "faiss-cpu>=1.8.0",
    "fire>=0.6.0",
    "flagembedding==1.3.5",
    "flake8>=7.0.0",
    "flash-attn==2.8.0.post2",
    "fsspec",
    "ftfy==6.3.1",
    "funasr>=1.1.4,<2.1.4",
    "google>=3.0.0",
    "gradio-client>=0.6.1",
    "gradio==5.49.1",
    "html2text>=2025.4.15,<2026.0.0",
    "httpx<0.28.0",
    "huggingface-hub>=0.23.1",
    "imageio-ffmpeg==0.6.0",
    "imageio==2.37.0",
    "infinity-emb==0.0.77",
    "jieba>=0.42.1",
    "lazyllm-llamafactory==0.9.4.dev2",
    "lazyllm-lmdeploy==0.10.2.dev0",
    "lazyllm-verl==0.3.2.dev2",
    "lightllm>=0.0.1,<0.0.2",
    "loralib",
    "mcp>=1.5.0",
    "modelscope==1.27.1",
    "nbconvert>=7.16.6,<8.16.6",
    "nltk>=3.8.1,<4.0.0",
    "numpy==1.26.4",
    "olefile>=0.47,<0.48",
    "openai-whisper",
    "openpyxl>=3.1.5,<4.0.0",
    "opensearch-py>=3.0.0",
    "optimum==1.24.0",
    "pandas>=2.2.2",
    "peft==0.17.1",
    "pluggy==1.6.0",
    "protobuf>=3.20.1",
    "psycopg2-binary>=2.9.9,<3.0.0",
    "pydub>=0.25.1,<0.26.1",
    "pymilvus>=2.4.11, <2.5.0",
    "pymongo>=4.12.1,<5.12.1",
    "pymysql>=1.1.1,<2.1.1",
    "pypdf>=5.0.0,<6.0.0",
    "pystemmer>=2.2.0.1,<3.0.0",
    "pytesseract>=0.3.13,<0.4.13",
    "pytest>=8.3.3,<9.3.3",
    "python-pptx>=1.0.2,<2.0.0",
    "qwen-vl-utils>=0.0.11,<0.0.12",
    "rank-bm25>=0.2.2",
    "rapidfuzz>=3.12.2,<4.12.2",
    "redis>=5.0.4",
    "redisvl>=0.1.3",
    "rotary-embedding-torch>=0.8.3,<0.9.3",
    "scikit-learn>=1.5.0",
    "sentence-transformers>=3.0.1,<4.0.1",
    "sentencepiece>=0.2.0,<0.3.0",
    "sortedcontainers>=2.4.0,<3.4.0",
    "spacy<=3.7.5",
    "sqlalchemy>=2.0.34,<3.0.0",
    "tenacity>=9.1.2,<10.0.0",
    "tensorboard-data-server>=0.7.2",
    "tensorboard>=2.16.2",
    "tiktoken>=0.7.0,<0.8.0",
    "timm>=1.0.8,<2.0.8",
    "tokenizers==0.22.0",
    "torch>=2.1.2",
    "torchvision>=0.16.2",
    "transformers==4.57.1",
    "typer>=0.12.5,<0.13.5",
    "vllm==0.10.1",
    "volcengine-python-sdk>=4.0.6",
    "wandb>=0.17.0",
    "zhipuai>=2.1.5.20250708",
]
[tool.lazyllm.extras_descriptions]
standard = "Install minimal dependencies for all LazyLLM features. Supports online model fine-tuning and inference, as well as offline model fine-tuning (requires LLaMA-Factory) and inference (requires vLLM)"
full = "Install all dependencies for LazyLLM, enabling every feature and advanced tools: automatic framework selection (AutoFinetune, AutoDeploy), additional offline inference engines (LightLLM), and extra training tools (AlpacaloraFinetune, CollieFinetune)"
alpaca-lora = "Install dependencies for the Alpaca-LoRA fine-tuning framework for local model training"
colie = "Install dependencies for the Collie fine-tuning framework for local model training"
llama-factory = "Install dependencies for LLaMA-Factory fine-tuning framework"
finetune-all = "Install all fine-tuning frameworks, including alpaca-lora, colie, and llama-factory"
vllm = "Install dependencies for vLLM local model inference framework"
lmdeploy = "Install dependencies for LMDeploy local model inference framework"
lightllm = "Install dependencies for LightLLM local model inference framework"
infinity = "Install dependencies for local embedding inference using the Infinity framework"
deploy-all = "Install all local inference frameworks, including LightLLM, vLLM, LMDeploy, and Infinity"
multimodal = "Install dependencies for multimodal features (speech generation, text-to-image, etc.)"
rag-advanced = "Install advanced RAG features, including vector database support and embedding fine-tuning"
agent-advanced = "Install advanced agent-related features with MCP support"
dev = "Install developer dependencies for code style checks and testing scripts"
online-advanced = "Install dependencies for online multimodal"

[tool.pytest.ini_options]
markers = [
    "run_on_change: skip test unless related files changed",
    "ignore_cache_on_change: use cache unless related files changed",
    "skip_on_win: mark tests to skip on Windows",
    "skip_on_mac: mark tests to skip on macOS",
    "skip_on_linux: mark tests to skip on Linux",
]
order_group_scope = "class"

