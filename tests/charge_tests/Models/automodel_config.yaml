#
# AutoModel charge tests 使用的示例配置。
# 这些条目覆盖了不同的推理模式，以便验证 AutoModel 在各种输入下的路由逻辑。
#

# 默认走本地可训练模型：框架信息触发 TrainableModule。
internlm-test:
  - framework: vllm
    source: local
    url: http://127.0.0.1:8000/v1/
    port: 8000
    deploy_config:
      max_model_len: 4096
      gpu_memory_utilization: 0.6

# SenseNova 在线服务：如果用户未提供 api_key，可通过环境变量补齐。
sensenova-model:
  - source: sensenova
    url: https://api.sensenova.com/v1/
    task: llm
  - source: sensenova
    api_key: config-api-key
    url: https://alt.sensenova.com/v1/

# 显式指定在线供应商（如 glm）。
glm-model:
  - source: glm
    url: https://glm.fake.endpoint/v1/

# 指定了框架但没有其它限制，仍然优先部署到本地框架。
trainable-model:
  - framework: vllm
    source: local
    url: http://127.0.0.1:2333/v1/
    port: 2333
    deploy_config:
      pipeline_parallel_size: 1
      tensor_parallel_size: 1

# 仅给出自定义 url/port，则构造成在线模块。
online-url-model:
  - url: http://custom.online.endpoint/v1/
    port: 9001

# 在 config 中直接提供线上凭证。
credential-model:
  - source: sensenova
    api_key: config-key
    url: https://credential.endpoint/v1/

# 只声明 source=sensenova；期望 AutoModel 从环境变量读取 key。
env-only-model:
  - source: sensenova
