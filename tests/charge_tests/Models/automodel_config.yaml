#
# Example config for AutoModel charge tests.
# Entries cover various routing scenarios for AutoModel.
#

# Prefer local trainable models when framework info exists.
internlm-test:
  - id: id-internlm-default
    name: internlm-alias
    framework: vllm
    source: local
    url: http://127.0.0.1:8000/v1/
    port: 8000
    deploy_config:
      max_model_len: 4096
      gpu_memory_utilization: 0.6

# SenseNova service; env variables can provide missing keys.
sensenova-model:
  - id: id-sensenova-default
    source: sensenova
    url: https://api.sensenova.com/v1/
    task: llm
  - id: id-sensenova-alt
    source: sensenova
    api_key: config-api-key
    url: https://alt.sensenova.com/v1/

# Explicitly specify online supplier (e.g. glm).
glm-model:
  - id: id-glm-default
    name: glm-remote
    source: glm
    url: https://glm.fake.endpoint/v1/

# Framework provided, still deploy to local TrainableModule.
trainable-model:
  - framework: vllm
    source: local
    url: http://127.0.0.1:2333/v1/
    port: 2333
    deploy_config:
      pipeline_parallel_size: 1
      tensor_parallel_size: 1

# Only url/port provided, route to OnlineModule.
online-url-model:
  - url: http://custom.online.endpoint/v1/
    port: 9001

# Config includes credential information for online service.
credential-model:
  - source: sensenova
    api_key: config-key
    url: https://credential.endpoint/v1/

# Only source=sensenova; expect env variable to provide key.
env-only-model:
  - source: sensenova
