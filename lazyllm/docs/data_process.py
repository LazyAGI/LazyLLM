# flake8: noqa E501
import importlib
from . import utils
import functools

add_chinese_doc = functools.partial(utils.add_chinese_doc, module=importlib.import_module('lazyllm.tools'))
add_english_doc = functools.partial(utils.add_english_doc, module=importlib.import_module('lazyllm.tools'))
add_example = functools.partial(utils.add_example, module=importlib.import_module('lazyllm.tools'))

add_chinese_doc('data.data_register', """\
æ•°æ®å¤„ç†ç®—å­æ³¨å†Œå™¨è£…é¥°å™¨ / å·¥å‚ï¼Œç”¨äºå°†å‡½æ•°æˆ–ç±»æ³¨å†Œä¸ºå¯å¤ç”¨çš„æ•°æ®å¤„ç†ç®—å­ã€‚

ç”¨æ³•ï¼š
     
- å¯ä»¥ç”¨æ¥æ³¨å†Œå•æ¡æ•°æ®å¤„ç†ç®—å­ï¼ˆå®ç° forward æ–¹æ³•æˆ–å‡½æ•°ï¼‰ã€‚
- å¯ä»¥ç”¨æ¥æ³¨å†Œæ‰¹å¤„ç†ç®—å­ï¼ˆå®ç° forward_batch_input æ–¹æ³•æˆ–å‡½æ•°ï¼‰ã€‚
- æ”¯æŒé€šè¿‡å‚æ•° rewrite_func æŒ‡å®šæ³¨å†Œæ—¶æ›¿æ¢æ¡†æ¶è°ƒç”¨çš„æ–¹æ³•ï¼ˆ'forward' æˆ– 'forward_batch_input'ï¼‰ã€‚

Args:
    name (str): æ³¨å†Œè·¯å¾„ï¼Œä¾‹å¦‚ 'data.mygroup'ï¼Œä¾¿äºæŒ‰ç»„æŸ¥æ‰¾ã€‚
    rewrite_func (str): å¯é€‰ï¼ŒæŒ‡å®šåœ¨æ³¨å†Œæ—¶æ¡†æ¶åº”å½“ä½¿ç”¨çš„æ‰§è¡Œæ¥å£ï¼ˆ'forward' æˆ– 'forward_batch_input'ï¼‰ã€‚
""")

add_english_doc('data.data_register', """\
Decorator / factory for registering data processing operators.

Usage:

- Register functions or classes that process single data items (implementing forward) or batches (implementing forward_batch_input).
- You may specify rewrite_func to indicate which interface to use ('forward' or 'forward_batch_input').

Args:
    name (str): registration path, e.g. 'data.mygroup', used to group operators.
    rewrite_func (str): optional, the method the framework should invoke ('forward' or 'forward_batch_input').
""")

add_example('data.data_register', """\
```python
from lazyllm.tools.data import data_register

Demo = data_register.new_group('Demo')
            
# register a simple batch function
@data_register('data.Demo', rewrite_func='forward_batch_input')
def my_batch_op(data, input_key='text'):
    for item in data:
        item[input_key] = item.get(input_key, '').strip()
    return data

# register a class-based operator
class MyOp(Demo):
    def forward(self, data):
        data['processed'] = True
        return data
```
""")

add_chinese_doc('data.LazyLLMDataBase', """\
æ•°æ®å¤„ç†ç®—å­åŸºç±»ã€‚ä¸ºæ³¨å†Œåˆ° data_register çš„ç®—å­æä¾›ç»Ÿä¸€è¡Œä¸ºï¼ŒåŒ…æ‹¬å¹¶å‘æ‰§è¡Œã€ç»“æœä¿å­˜/æ¢å¤ã€è¿›åº¦è®°å½•å’Œé”™è¯¯æ”¶é›†ã€‚

ä¸»è¦æ–¹æ³•å’Œè¡Œä¸ºï¼š

- forward(self, input, **kwargs): å¤„ç†å•æ¡æ•°æ®ï¼ˆå­ç±»/å‡½æ•°å®ç°ï¼‰ã€‚
- forward_batch_input(self, inputs, **kwargs): å¤„ç†æ‰¹é‡æ•°æ®å¹¶è¿”å›æœ€ç»ˆç»“æœï¼ˆå­ç±»/å‡½æ•°å®ç°ï¼‰ã€‚
- __call__(self, inputs): ç»Ÿä¸€å…¥å£ï¼Œä¼šæ ¹æ®å­ç±»æ˜¯å¦å®ç° forward æˆ– forward_batch_input é€‰æ‹©æ‰§è¡Œé€»è¾‘ï¼›æ”¯æŒå¹¶å‘æ‰§è¡Œã€æ–­ç‚¹ç»­ä¼ å’Œä¿å­˜ç»“æœã€‚
- set_output(self, path): è®¾ç½®å¯¼å‡ºè·¯å¾„ï¼Œè°ƒç”¨å __call__ è¿”å›å¯¼å‡ºæ–‡ä»¶è·¯å¾„è€Œä¸æ˜¯å†…å­˜ç»“æœã€‚

æ„é€ å‡½æ•°å‚æ•°:

- _concurrency_mode (str): å¹¶å‘æ¨¡å¼ï¼Œ'process'|'thread'|'single'ã€‚
- _save_data (bool): æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœåˆ°ç£ç›˜ä»¥ä¾¿ Resumeã€‚
- _max_workers (int|None): æœ€å¤§å¹¶å‘å·¥ä½œè¿›ç¨‹/çº¿ç¨‹æ•°ï¼ŒNone è¡¨ç¤ºä½¿ç”¨é»˜è®¤ã€‚
- _ignore_errors (bool): æ˜¯å¦å¿½ç•¥ä»»åŠ¡å¼‚å¸¸ã€‚
- **kwargs (dict): å…¶å®ƒä¼ é€’ç»™ç®—å­çš„å‚æ•°ã€‚

é…ç½®é¡¹ï¼ˆé€šè¿‡ lazyllm.configï¼‰:

- data_process_path (str): å­˜å‚¨å¤„ç†ç»“æœçš„æ ¹è·¯å¾„ã€‚
- data_process_resume (bool): æ˜¯å¦å¼€å¯ Resume åŠŸèƒ½ï¼Œä»è¿›åº¦æ–‡ä»¶ç»§ç»­å¤„ç†ã€‚
""")

add_english_doc('data.LazyLLMDataBase', """\
Base class for data processing operators registered via data_register.
Provides concurrency, result persistence/resume, progress tracking, and error collection.

Key methods:

- forward(self, input, **kwargs): implement single-item processing.
- forward_batch_input(self, inputs, **kwargs): implement batch processing and return results.
- __call__(self, inputs): unified entry point; decides execution mode based on implemented methods and handles concurrency, resume and saving.
- set_output(self, path): set export path; when set, __call__ writes results to a file and returns the file path.

Constructor args:

- _concurrency_mode (str): concurrency mode, one of 'process'|'thread'|'single'.
- _save_data (bool): whether to persist intermediate results for resume.
- _max_workers (int|None): maximum workers for concurrency, None means default.
- _ignore_errors (bool): whether to ignore exceptions in tasks.
- **kwargs (dict): additional operator arguments.

Config keys (via lazyllm.config):

- data_process_path (str): root folder to store pipeline outputs.
- data_process_resume (bool): enable resume from previous progress.
""")

add_example('data.LazyLLMDataBase', """\
```python
from lazyllm.tools.data import LazyLLMDataBase

# simple usage: subclass and implement forward
class EchoOp(LazyLLMDataBase):
    def forward(self, data):
        return {'text': data.get('text', '')}

op = EchoOp(_save_data=True)
res = op([{'text': 'hello'}])  # returns list or exported path depending on set_output
```
""")

add_chinese_doc('data.LazyLLMDataBase.set_output', """\
è®¾ç½®è¾“å‡ºè·¯å¾„ï¼Œç”¨äºæŠŠæœ€ç»ˆç»“æœå¯¼å‡ºä¸º jsonl æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„ã€‚

Args:
    output_path (str): æ–‡ä»¶å¤¹è·¯å¾„æˆ–å…·ä½“ .jsonl æ–‡ä»¶è·¯å¾„ã€‚è‹¥ä¸ºæ–‡ä»¶å¤¹ï¼Œåˆ™åœ¨è¯¥æ–‡ä»¶å¤¹ä¸‹åˆ›å»ºä»¥ç±»åå‘½åçš„ jsonl æ–‡ä»¶ã€‚

è¡Œä¸ºï¼š

- å¦‚æœä¼ å…¥çš„æ˜¯æ–‡ä»¶å¤¹è·¯å¾„ï¼Œåˆ™åœ¨è¯¥æ–‡ä»¶å¤¹ä¸‹åˆ›å»ºä»¥ç±»åå‘½åçš„ jsonl æ–‡ä»¶ã€‚
- å¦‚æœä¼ å…¥çš„æ˜¯ä»¥ .jsonl ç»“å°¾çš„è·¯å¾„ï¼Œåˆ™ç›´æ¥å†™å…¥è¯¥æ–‡ä»¶ï¼ˆå¿…è¦æ—¶ä¼šåˆ›å»ºç›®å½•ï¼‰ã€‚
- è¿”å›å†™å…¥çš„ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²ã€‚
""")

add_english_doc('data.LazyLLMDataBase.set_output', """\
Set output path for exporting final results to a JSONL file and return the file path.

Args:
    output_path (str): directory path or concrete .jsonl file path. If a directory is provided, a file named <ClassName>.jsonl will be created inside it.

Behavior:
- If a folder path is provided, a file named <ClassName>.jsonl will be created in that folder.
- If a .jsonl file path is provided, results will be written to that file (directories created as needed).
- Returns the absolute path of the exported file.
""")

add_example('data.LazyLLMDataBase.set_output', """\
```python
from lazyllm.tools.data import Demo2

# export to a directory (will create DemoClass.jsonl)
op = Demo2.rich_content(input_key='text').set_output('./out_dir')
path = op([{'text': 'sample'}])
print(path)  # ./out_dir/RichContent.jsonl or similar

# export to a specific file
op = Demo2.rich_content(input_key='text').set_output('./out_dir/results.jsonl')
path = op([{'text': 'sample'}])
print(path)  # ./out_dir/results.jsonl
```
""")  

add_chinese_doc('data.LazyLLMDataBase.forward', """\
å­ç±»éœ€è¦å®ç°çš„æ–¹æ³•ï¼Œå¤„ç†å•æ¡æ•°æ®ã€‚è¿”å›å€¼æ”¯æŒï¼š

- dict: è¡¨ç¤ºå¤„ç†åçš„å•æ¡ç»“æœã€‚
- list: è¡¨ç¤ºå°†ä¸€æ¡è¾“å…¥å±•å¼€ä¸ºå¤šæ¡è¾“å‡ºã€‚
- None: è¡¨ç¤ºä¿ç•™åŸå§‹è¾“å…¥ï¼ˆä¸ä¿®æ”¹ï¼‰ã€‚
- æŠ›å‡ºå¼‚å¸¸æˆ–è¿”å›é”™è¯¯å¯¹è±¡ä¼šè¢«è®°å½•åˆ°é”™è¯¯æ–‡ä»¶å¹¶è·³è¿‡ï¼ˆä¾èµ–é…ç½®å’Œè°ƒç”¨è€…ï¼‰ã€‚

Args:
    input (dict): å•æ¡è¾“å…¥æ•°æ®å­—å…¸ã€‚
    **kwargs (dict): å…¶å®ƒç”¨æˆ·ä¼ å…¥çš„å‚æ•°ã€‚
""")

add_english_doc('data.LazyLLMDataBase.forward', """\
Method to implement in subclasses for single-item processing. Supported return types:

- dict: processed single result.
- list: expand one input into multiple outputs.
- None: keep the original input unchanged.
Exceptions or error returns are recorded to the error file and typically skipped from valid results.

Args:
    input (dict): a single input data dict.
    **kwargs (dict): additional user-provided arguments.
""")

add_example('data.LazyLLMDataBase.forward', """\
```python
from lazyllm.tools.data import LazyLLMDataBase

class MyOp(LazyLLMDataBase):
    def forward(self, data):
        # return dict or list or None
        return {'text': data.get('text', '').upper()}

op = MyOp()
print(op([{'text': 'a'}]))
```
""")

add_chinese_doc('data.LazyLLMDataBase.forward_batch_input', """\
å­ç±»å¯å®ç°çš„æ‰¹é‡å¤„ç†æ–¹æ³•ï¼Œç”¨äºåœ¨éé€æ¡å¹¶å‘åœºæ™¯ä¸‹ç›´æ¥æ¥æ”¶æ•´ä¸ªè¾“å…¥åˆ—è¡¨å¹¶è¿”å›æœ€ç»ˆç»“æœåˆ—è¡¨ï¼ˆå¯ç”¨äºè‡ªå®šä¹‰æ‰¹é‡é€»è¾‘æˆ–å¤–éƒ¨æœåŠ¡ä¸€æ¬¡æ€§å¤„ç†ï¼‰ã€‚

Args:
    inputs (list[dict]): è¾“å…¥æ•°æ®åˆ—è¡¨ã€‚
    **kwargs (dict): å…¶å®ƒç”¨æˆ·ä¼ å…¥çš„å‚æ•°ã€‚
""")

add_english_doc('data.LazyLLMDataBase.forward_batch_input', """\
Optional batch-processing method for subclasses. Receives the whole input list and returns a final list of results. Useful for custom batching or single-call external services.

Args:
    inputs (list[dict]): list of input data dicts.
    **kwargs (dict): additional user-provided arguments.
""")

add_example('data.LazyLLMDataBase.forward_batch_input', """\
```python
from lazyllm.tools.data import LazyLLMDataBase

class BatchOp(LazyLLMDataBase):
    def forward_batch_input(self, inputs):
        # implement batch processing and return a list
        return [{'text': i.get('text', '').lower()} for i in inputs]

op = BatchOp()
print(op([{'text': 'A'}, {'text': 'B'}]))
```
""")

add_chinese_doc('data.operators.demo_ops.process_uppercase', """\
å°†è¾“å…¥æ–‡æœ¬å­—æ®µè½¬æ¢ä¸ºå¤§å†™ã€‚é€‚ç”¨äºå•æ¡å¤„ç†å‡½æ•°æ³¨å†Œï¼ˆforwardï¼‰ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.demo_ops.process_uppercase', """\
Convert the input text field to uppercase. Intended as a single-item processing function.

Args:
    data (dict): a dict representing a single data item.
    input_key (str): key name of the text field, default 'content'.
""")

add_example('data.operators.demo_ops.process_uppercase', """\
```python
from lazyllm.tools.data.operators.demo_ops import process_uppercase

op = process_uppercase(input_key='text')
print(op({'text': 'hello'}))  # {'text': 'HELLO'}
```
""")

add_chinese_doc('data.operators.demo_ops.build_pre_suffix', """\
å¯¹è¾“å…¥åˆ—è¡¨ä¸­æ¯é¡¹åœ¨æŒ‡å®šå­—æ®µå‰åæ·»åŠ å‰ç¼€å’Œåç¼€ã€‚æ­¤ç®—å­ä»¥æ‰¹å¤„ç†å‡½æ•°æ³¨å†Œï¼ˆforward_batch_inputï¼‰ã€‚

Args:
    data (list[dict]): è¾“å…¥åˆ—è¡¨
    input_key (str): æ–‡æœ¬å­—æ®µå
    prefix (str): è¦æ·»åŠ çš„å‰ç¼€
    suffix (str): è¦æ·»åŠ çš„åç¼€
""")

add_english_doc('data.operators.demo_ops.build_pre_suffix', """\
Add a prefix and suffix to the specified field of each item in the input list. Registered as a batch operator.

Args:
    data (list[dict]): list of dicts
    input_key (str): key name of the text field
    prefix (str): string to add before the field
    suffix (str): string to add after the field
""")

add_example('data.operators.demo_ops.build_pre_suffix', """\
```python
from lazyllm.tools.data.operators.demo_ops import build_pre_suffix

op = build_pre_suffix(input_key='text', prefix='Hello, ', suffix='!')
print(op([{'text': 'world'}]))
# [{'text': 'Hello, world!'}]
```
""")

add_chinese_doc('data.operators.demo_ops.AddSuffix', """\
é€šè¿‡ç±»æ–¹å¼å®ç°çš„ç®—å­ï¼Œä¸ºæŒ‡å®šå­—æ®µæ·»åŠ åç¼€ã€‚æ”¯æŒå¹¶å‘é…ç½®ï¼ˆé€šè¿‡æ„é€ å‚æ•°ï¼‰ã€‚

Args:
    suffix (str): è¦æ·»åŠ çš„åç¼€
    input_key (str): æ–‡æœ¬å­—æ®µå
    _max_workers (int|None): å¯é€‰ï¼Œæœ€å¤§å¹¶å‘æ•°
    _concurrency_mode (str): å¯é€‰ï¼Œå¹¶å‘æ¨¡å¼
    _save_data (bool): å¯é€‰ï¼Œæ˜¯å¦ä¿å­˜ç»“æœ
""")

add_english_doc('data.operators.demo_ops.AddSuffix', """\
Class-based operator that appends a suffix to a specified field. Supports concurrency configuration via constructor args.

Args:
    suffix (str): suffix string to append
    input_key (str): key name of the text field
    _max_workers (int|None): optional max concurrency
    _concurrency_mode (str): optional concurrency mode
    _save_data (bool): optional whether to persist results
""")

add_example('data.operators.demo_ops.AddSuffix', """\
```python
from lazyllm.tools.data.operators.demo_ops import AddSuffix

op = AddSuffix(suffix='!!!', input_key='text', _max_workers=2)
print(op([{'text': 'wow'}]))  # [{'text': 'wow!!!'}]
```
""")

add_chinese_doc('data.operators.demo_ops.rich_content', """\
å°†å•æ¡è¾“å…¥æ‹†åˆ†ä¸ºå¤šæ¡è¾“å‡ºï¼Œç”Ÿæˆå¯Œå†…å®¹è¡¨ç¤ºï¼ˆåŸå§‹ + è‹¥å¹²æ´¾ç”Ÿï¼‰ã€‚é€‚ç”¨äºè¿”å› list çš„ forwardã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µå
""")

add_english_doc('data.operators.demo_ops.rich_content', """\
Split a single input into multiple outputs (original + derived parts). Implemented as a forward that returns a list.

Args:
    data (dict): single data dict
    input_key (str): key name of the text field
""")

add_example('data.operators.demo_ops.rich_content', """\
```python
from lazyllm.tools.data.operators.demo_ops import rich_content

op = rich_content(input_key='text')
print(op({'text': 'This is a test.'}))
# [
#   {'text': 'This is a test.'},
#   {'text': 'This is a test. - part 1'},
#   {'text': 'This is a test. - part 2'}
# ]
```
""")

add_chinese_doc('data.operators.demo_ops.error_prone_op', """\
ä¸€ä¸ªç”¨äºæµ‹è¯•çš„ç®—å­ï¼šåœ¨ç‰¹å®šè¾“å…¥ï¼ˆcontent == 'fail'ï¼‰æ—¶æŠ›å‡ºå¼‚å¸¸ï¼Œå¦åˆ™è¿”å›å¤„ç†åçš„å­—å…¸ç»“æœã€‚ç”¨äºéªŒè¯é”™è¯¯æ”¶é›†ä¸è·³è¿‡é€»è¾‘ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µå
""")

add_english_doc('data.operators.demo_ops.error_prone_op', """\
A test operator that raises an exception for specific input (content == 'fail') and otherwise returns a processed dict.
Used to validate error collection and skipping behavior.

Args:
    data (dict): single data dict
    input_key (str): key name of the text field
""")

add_example('data.operators.demo_ops.error_prone_op', """\
```python
from lazyllm.tools.data.operators.demo_ops import error_prone_op

op = error_prone_op(input_key='text', _save_data=True, _concurrency_mode='single')
res = op([{'text': 'ok'}, {'text': 'fail'}, {'text': 'ok2'}])
# valid results skip the failed item; error details written to error file
```
""")

# refine_op
add_chinese_doc('data.operators.refine_op.remove_extra_spaces', """\
å°†æŒ‡å®šå­—æ®µä¸­çš„å¤šä½™ç©ºç™½ï¼ˆå¤šä¸ªç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦ï¼‰å½’ä¸€åŒ–ä¸ºå•ä¸ªç©ºæ ¼ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.refine_op.remove_extra_spaces', """\
Normalize whitespace by collapsing multiple spaces, newlines and tabs into single spaces.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
""")

add_example('data.operators.refine_op.remove_extra_spaces', """\
```python
from lazyllm.tools.data import refine

func = refine.remove_extra_spaces(input_key='content')
inputs = [{'content': 'hello   world\\\\n\\\\n  foo\\\\tbar'}]
res = func(inputs)
print(res)
# [{'content': 'hello world foo bar'}]
```
""")

add_chinese_doc('data.operators.refine_op.remove_emoji', """\
ç§»é™¤æŒ‡å®šå­—æ®µä¸­çš„ emoji å­—ç¬¦ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.refine_op.remove_emoji', """\
Remove emoji characters from the specified text field.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
""")

add_example('data.operators.refine_op.remove_emoji', """\
```python
from lazyllm.tools.data import refine

func = refine.remove_emoji(input_key='content')
inputs = [{'content': 'Hello ğŸ˜Š World ğŸŒ!'}]
res = func(inputs)
print(res)
# [{'content': 'Hello  World !'}]
```
""")

add_chinese_doc('data.operators.refine_op.remove_html_url', """\
ç§»é™¤æŒ‡å®šå­—æ®µä¸­çš„ HTTP/HTTPS é“¾æ¥å’Œ HTML æ ‡ç­¾ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.refine_op.remove_html_url', """\
Remove HTTP/HTTPS URLs and HTML tags from the specified text field.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
""")

add_example('data.operators.refine_op.remove_html_url', """\
```python
from lazyllm.tools.data import refine

func = refine.remove_html_url(input_key='content')
inputs = [{'content': 'Check https://example.com and <b>bold</b>'}]
res = func(inputs)
print(res)
# [{'content': 'Check  and bold'}]
```
""")

add_chinese_doc('data.operators.refine_op.remove_html_entity', """\
ç§»é™¤æŒ‡å®šå­—æ®µä¸­çš„ HTML å®ä½“ï¼ˆå¦‚ &nbsp;ã€&lt;ã€&amp; ç­‰ï¼‰ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.refine_op.remove_html_entity', """\
Remove HTML entities (e.g. &nbsp;, &lt;, &amp;) from the specified text field.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
""")

add_example('data.operators.refine_op.remove_html_entity', """\
```python
from lazyllm.tools.data import refine

func = refine.remove_html_entity(input_key='content')
inputs = [{'content': 'Hello&nbsp;World &amp; &lt;tag&gt;'}]
res = func(inputs)
print(res)
# [{'content': 'HelloWorld  tag'}]
```
""")

# token_chunker
add_chinese_doc('data.operators.token_chunker.TokenChunker', """\
æŒ‰ token æ•°é‡å°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºå¤šä¸ªå—ã€‚å…ˆæŒ‰æ®µè½åˆ†éš”ï¼Œå†æŒ‰å¥å­ç»†åˆ‡ï¼Œä¿è¯æ¯å—ä¸è¶…è¿‡ max_tokensï¼Œè¿‡çŸ­å—å¯ä¸¢å¼ƒã€‚

Args:
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    model_path (str|None): tokenizer æ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ Qwen2.5-0.5B-Instruct
    max_tokens (int): æ¯å—æœ€å¤§ token æ•°ï¼Œé»˜è®¤ 1024
    min_tokens (int): æ¯å—æœ€å° token æ•°ï¼Œä½äºæ­¤å€¼çš„å—å¯èƒ½è¢«ä¸¢å¼ƒï¼Œé»˜è®¤ 200
    _concurrency_mode (str): å¯é€‰ï¼Œå¹¶å‘æ¨¡å¼
    _max_workers (int|None): å¯é€‰ï¼Œæœ€å¤§å¹¶å‘æ•°
""")

add_english_doc('data.operators.token_chunker.TokenChunker', """\
Split long text into chunks by token count. Splits by paragraph first, then by sentence.
Ensures each chunk does not exceed max_tokens; chunks below min_tokens may be discarded.

Args:
    input_key (str): key of the text field, default 'content'
    model_path (str|None): path to tokenizer model, default Qwen2.5-0.5B-Instruct
    max_tokens (int): max tokens per chunk, default 1024
    min_tokens (int): min tokens per chunk, smaller chunks may be discarded, default 200
    _concurrency_mode (str): optional concurrency mode
    _max_workers (int|None): optional max concurrency
""")

add_example('data.operators.token_chunker.TokenChunker', """\
```python
from lazyllm.tools.data import chunker

func = chunker.TokenChunker(input_key='content', max_tokens=50, min_tokens=10)
inputs = [{'content': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚' * 20, 'meta_data': {'source': 'doc_1'}}]
res = func(inputs)
print(res)
# [{'uid': '...', 'content': '...', 'meta_data': {'source': 'doc_1', 'index': 0, 'total': N, 'length': ...}}, ...]
```
""")

# filter_op
add_chinese_doc('data.operators.filter_op.LanguageFilter', """\
ä½¿ç”¨ FastText è¿›è¡Œè¯­è¨€è¯†åˆ«ï¼Œä»…ä¿ç•™æŒ‡å®šè¯­è¨€çš„æ–‡æœ¬ã€‚

Args:
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    target_language (str|list): ç›®æ ‡è¯­è¨€ä»£ç ï¼Œå¦‚ 'zho_Hans'ã€'eng_Latn'
    threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤ 0.6
    model_path (str|None): FastText æ¨¡å‹è·¯å¾„
    _concurrency_mode (str): å¯é€‰ï¼Œå¹¶å‘æ¨¡å¼
""")

add_english_doc('data.operators.filter_op.LanguageFilter', """\
Filter text by language using FastText. Keeps only texts in the specified language(s).

Args:
    input_key (str): key of the text field, default 'content'
    target_language (str|list): target language code(s), e.g. 'zho_Hans', 'eng_Latn'
    threshold (float): confidence threshold, default 0.6
    model_path (str|None): path to FastText model
    _concurrency_mode (str): optional concurrency mode
""")

add_example('data.operators.filter_op.LanguageFilter', """\
```python
from lazyllm.tools.data import filter

func = filter.LanguageFilter(input_key='content', target_language='zho_Hans', threshold=0.3)
inputs = [{'content': 'è¿™æ˜¯ä¸€æ®µä¸­æ–‡æ–‡æœ¬ã€‚'}, {'content': 'This is English.'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯ä¸€æ®µä¸­æ–‡æ–‡æœ¬ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.MinHashDeduplicateFilter', """\
ä½¿ç”¨ MinHash LSH å»é™¤è¿‘ä¼¼é‡å¤æ–‡æœ¬ï¼Œæ‰¹å¤„ç†æ—¶ä¿ç•™é¦–æ¬¡å‡ºç°çš„æ–‡æœ¬ã€‚

Args:
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    threshold (float): ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ 0.85
    num_perm (int): MinHash æ’åˆ—æ•°ï¼Œé»˜è®¤ 128
    use_n_gram (bool): æ˜¯å¦ä½¿ç”¨ n-gramï¼Œé»˜è®¤ True
    ngram (int): n-gram é•¿åº¦ï¼Œé»˜è®¤ 5
""")

add_english_doc('data.operators.filter_op.MinHashDeduplicateFilter', """\
Remove near-duplicate texts using MinHash LSH. For batch input, keeps first occurrence of each unique text.

Args:
    input_key (str): key of the text field, default 'content'
    threshold (float): similarity threshold, default 0.85
    num_perm (int): number of MinHash permutations, default 128
    use_n_gram (bool): use n-gram, default True
    ngram (int): n-gram size, default 5
""")

add_example('data.operators.filter_op.MinHashDeduplicateFilter', """\
```python
from lazyllm.tools.data import filter

func = filter.MinHashDeduplicateFilter(input_key='content', threshold=0.85)
inputs = [{'uid': '0', 'content': 'è¿™æ˜¯ç¬¬ä¸€æ®µä¸åŒçš„å†…å®¹ã€‚'}, {'uid': '1', 'content': 'è¿™æ˜¯ç¬¬ä¸€æ®µä¸åŒçš„å†…å®¹ã€‚'}]
res = func(inputs)
print(res)
# [{'uid': '0', 'content': 'è¿™æ˜¯ç¬¬ä¸€æ®µä¸åŒçš„å†…å®¹ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.BlocklistFilter', """\
ä½¿ç”¨ AC è‡ªåŠ¨æœºå¤šæ¨¡å¼åŒ¹é…è¿‡æ»¤åŒ…å«æ•æ„Ÿè¯/è¿ç¦è¯è¶…è¿‡é˜ˆå€¼çš„æ–‡æœ¬ã€‚

Args:
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    blocklist (list|None): è¿ç¦è¯åˆ—è¡¨
    blocklist_path (str|None): è¿ç¦è¯æ–‡ä»¶è·¯å¾„
    language (str): è¯­è¨€ï¼Œ'zh' æˆ– 'en'ï¼Œé»˜è®¤ 'zh'
    threshold (int): å…è®¸å‡ºç°çš„è¿ç¦è¯æœ€å¤§æ•°é‡ï¼Œé»˜è®¤ 1
    _concurrency_mode (str): å¯é€‰ï¼Œå¹¶å‘æ¨¡å¼
""")

add_english_doc('data.operators.filter_op.BlocklistFilter', """\
Filter text containing more than threshold blocked words using Aho-Corasick automaton.

Args:
    input_key (str): key of the text field, default 'content'
    blocklist (list|None): list of blocked words
    blocklist_path (str|None): path to blocklist file
    language (str): language, 'zh' or 'en', default 'zh'
    threshold (int): max allowed occurrences of blocked words, default 1
    _concurrency_mode (str): optional concurrency mode
""")

add_example('data.operators.filter_op.BlocklistFilter', """\
```python
from lazyllm.tools.data import filter

func = filter.BlocklistFilter(input_key='content', blocklist=['æ•æ„Ÿ', 'è¿ç¦'], threshold=0)
inputs = [{'content': 'è¿™æ˜¯æ­£å¸¸çš„æ–‡æœ¬å†…å®¹ã€‚'}, {'content': 'è¿™é‡ŒåŒ…å«æ•æ„Ÿè¯ã€‚'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯æ­£å¸¸çš„æ–‡æœ¬å†…å®¹ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.SymbolRatioFilter', """\
è¿‡æ»¤æŒ‡å®šç¬¦å·ï¼ˆå¦‚ #ã€...ã€â€¦ï¼‰å æ¯”è¿‡é«˜çš„æ–‡æœ¬ã€‚

Args:
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_ratio (float): ç¬¦å·ä¸è¯æ•°æœ€å¤§æ¯”ä¾‹ï¼Œé»˜è®¤ 0.3
    symbols (list|None): è¦ç»Ÿè®¡çš„ç¬¦å·åˆ—è¡¨ï¼Œé»˜è®¤ ['#', '...', 'â€¦']
    _concurrency_mode (str): å¯é€‰ï¼Œå¹¶å‘æ¨¡å¼
""")

add_english_doc('data.operators.filter_op.SymbolRatioFilter', """\
Filter text with too high ratio of specified symbols (e.g. #, ..., â€¦) to words.

Args:
    input_key (str): key of the text field, default 'content'
    max_ratio (float): max ratio of symbols to words, default 0.3
    symbols (list|None): symbols to count, default ['#', '...', 'â€¦']
    _concurrency_mode (str): optional concurrency mode
""")

add_example('data.operators.filter_op.SymbolRatioFilter', """\
```python
from lazyllm.tools.data import filter

func = filter.SymbolRatioFilter(input_key='content', max_ratio=0.3)
inputs = [{'content': 'Normal text without symbols'}, {'content': '### ... â€¦ ###'}]
res = func(inputs)
print(res)
# [{'content': 'Normal text without symbols'}]
```
""")

add_chinese_doc('data.operators.filter_op.StopWordFilter', """\
è¿‡æ»¤åœç”¨è¯å æ¯”è¿‡é«˜çš„æ–‡æœ¬ï¼ˆå¦‚å‡ ä¹å…¨ä¸ºã€Œçš„äº†å‘¢ã€çš„æ— æ•ˆå†…å®¹ï¼‰ã€‚

Args:
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_ratio (float): åœç”¨è¯æœ€å¤§å æ¯”ï¼Œè¶…è¿‡åˆ™è¿‡æ»¤ï¼Œé»˜è®¤ 0.5
    use_tokenizer (bool): æ˜¯å¦ä½¿ç”¨åˆ†è¯ï¼Œé»˜è®¤ True
    language (str): è¯­è¨€ï¼Œ'zh' æˆ– 'en'ï¼Œé»˜è®¤ 'zh'
    _concurrency_mode (str): å¯é€‰ï¼Œå¹¶å‘æ¨¡å¼
""")

add_english_doc('data.operators.filter_op.StopWordFilter', """\
Filter text with too high stopword ratio (e.g. invalid content mostly stopwords).

Args:
    input_key (str): key of the text field, default 'content'
    max_ratio (float): max stopword ratio, filter if exceeded, default 0.5
    use_tokenizer (bool): use tokenizer, default True
    language (str): language, 'zh' or 'en', default 'zh'
    _concurrency_mode (str): optional concurrency mode
""")

add_example('data.operators.filter_op.StopWordFilter', """\
```python
from lazyllm.tools.data import filter

func = filter.StopWordFilter(input_key='content', max_ratio=0.5, language='zh')
inputs = [{'content': 'è¿™æ˜¯ä¸€æ®µåŒ…å«å®é™…å†…å®¹çš„æ­£å¸¸æ–‡æœ¬ã€‚'}, {'content': 'çš„äº†å—å‘¢å§å•Š'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯ä¸€æ®µåŒ…å«å®é™…å†…å®¹çš„æ­£å¸¸æ–‡æœ¬ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.CapitalWordFilter', """\
è¿‡æ»¤å…¨å¤§å†™å•è¯å æ¯”è¿‡é«˜çš„æ–‡æœ¬ã€‚

Args:
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_ratio (float): å…¨å¤§å†™å•è¯æœ€å¤§å æ¯”ï¼Œé»˜è®¤ 0.5
    use_tokenizer (bool): æ˜¯å¦ä½¿ç”¨åˆ†è¯ï¼Œé»˜è®¤ False
    _concurrency_mode (str): å¯é€‰ï¼Œå¹¶å‘æ¨¡å¼
""")

add_english_doc('data.operators.filter_op.CapitalWordFilter', """\
Filter text with too high ratio of all-caps words.

Args:
    input_key (str): key of the text field, default 'content'
    max_ratio (float): max ratio of all-caps words, default 0.5
    use_tokenizer (bool): use tokenizer, default False
    _concurrency_mode (str): optional concurrency mode
""")

add_example('data.operators.filter_op.CapitalWordFilter', """\
```python
from lazyllm.tools.data import filter

func = filter.CapitalWordFilter(input_key='content', max_ratio=0.5)
inputs = [{'content': 'Normal text with Some Capitals'}, {'content': 'MOSTLY UPPERCASE'}]
res = func(inputs)
print(res)
# [{'content': 'Normal text with Some Capitals'}]
```
""")

add_chinese_doc('data.operators.filter_op.word_count_filter', """\
æŒ‰è¯/å­—ç¬¦æ•°é‡è¿‡æ»¤ï¼šä¸­æ–‡æŒ‰å­—ç¬¦æ•°ï¼Œè‹±æ–‡æŒ‰å•è¯æ•°ï¼Œä¿ç•™åœ¨ [min_words, max_words) èŒƒå›´å†…çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    min_words (int): æœ€å°è¯æ•°ï¼Œé»˜è®¤ 10
    max_words (int): æœ€å¤§è¯æ•°ï¼Œé»˜è®¤ 10000
    language (str): è¯­è¨€ï¼Œ'zh' æˆ– 'en'ï¼Œé»˜è®¤ 'zh'
""")

add_english_doc('data.operators.filter_op.word_count_filter', """\
Filter by word/char count: Chinese by char count, English by word count. Keeps text in [min_words, max_words).

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    min_words (int): min count, default 10
    max_words (int): max count, default 10000
    language (str): language, 'zh' or 'en', default 'zh'
""")

add_example('data.operators.filter_op.word_count_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.word_count_filter(input_key='content', min_words=5, max_words=20, language='zh')
inputs = [{'content': 'çŸ­æ–‡æœ¬'}, {'content': 'è¿™æ˜¯ä¸€æ®µé€‚ä¸­é•¿åº¦çš„ä¸­æ–‡æ–‡æœ¬å†…å®¹ã€‚'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯ä¸€æ®µé€‚ä¸­é•¿åº¦çš„ä¸­æ–‡æ–‡æœ¬å†…å®¹ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.colon_end_filter', """\
è¿‡æ»¤ä»¥å†’å·ç»“å°¾çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.filter_op.colon_end_filter', """\
Filter text ending with colon.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
""")

add_example('data.operators.filter_op.colon_end_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.colon_end_filter(input_key='content')
inputs = [{'content': 'è¿™æ˜¯æ­£å¸¸ç»“å°¾ã€‚'}, {'content': 'è¿™æ˜¯å†’å·ç»“å°¾ï¼š'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯æ­£å¸¸ç»“å°¾ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.sentence_count_filter', """\
æŒ‰å¥å­æ•°é‡è¿‡æ»¤ï¼Œä¿ç•™åœ¨ [min_sentences, max_sentences] èŒƒå›´å†…çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    min_sentences (int): æœ€å°‘å¥å­æ•°ï¼Œé»˜è®¤ 3
    max_sentences (int): æœ€å¤šå¥å­æ•°ï¼Œé»˜è®¤ 1000
    language (str): è¯­è¨€ï¼Œ'zh' æˆ– 'en'ï¼Œé»˜è®¤ 'zh'
""")

add_english_doc('data.operators.filter_op.sentence_count_filter', """\
Filter by sentence count. Keeps text with sentences in [min_sentences, max_sentences].

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    min_sentences (int): min sentence count, default 3
    max_sentences (int): max sentence count, default 1000
    language (str): language, 'zh' or 'en', default 'zh'
""")

add_example('data.operators.filter_op.sentence_count_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.sentence_count_filter(input_key='content', min_sentences=2, max_sentences=10, language='zh')
inputs = [{'content': 'å•å¥ã€‚'}, {'content': 'ç¬¬ä¸€å¥ã€‚ç¬¬äºŒå¥ã€‚'}]
res = func(inputs)
print(res)
# [{'content': 'ç¬¬ä¸€å¥ã€‚ç¬¬äºŒå¥ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.ellipsis_end_filter', """\
è¿‡æ»¤ä»¥çœç•¥å·ï¼ˆ...ã€â€¦ã€â€¦â€¦ï¼‰ç»“å°¾çš„è¡Œå æ¯”è¿‡é«˜çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_ratio (float): ä»¥çœç•¥å·ç»“å°¾çš„è¡Œæœ€å¤§å æ¯”ï¼Œé»˜è®¤ 0.3
""")

add_english_doc('data.operators.filter_op.ellipsis_end_filter', """\
Filter text with too many lines ending in ellipsis (...ã€â€¦ã€â€¦â€¦).

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    max_ratio (float): max ratio of lines ending with ellipsis, default 0.3
""")

add_example('data.operators.filter_op.ellipsis_end_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.ellipsis_end_filter(input_key='content', max_ratio=0.3)
inputs = [{'content': 'ç¬¬ä¸€è¡Œã€‚\\\\nç¬¬äºŒè¡Œã€‚\\\\nç¬¬ä¸‰è¡Œã€‚'}, {'content': 'ç¬¬ä¸€è¡Œ...\\\\nç¬¬äºŒè¡Œ...'}]
res = func(inputs)
print(res)
# [{'content': 'ç¬¬ä¸€è¡Œã€‚\\\\nç¬¬äºŒè¡Œã€‚\\\\nç¬¬ä¸‰è¡Œã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.null_content_filter', """\
è¿‡æ»¤ç©ºå†…å®¹æˆ–ä»…ç©ºç™½å­—ç¬¦çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.filter_op.null_content_filter', """\
Filter null or whitespace-only content.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
""")

add_example('data.operators.filter_op.null_content_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.null_content_filter(input_key='content')
inputs = [{'content': 'Valid content'}, {'content': ''}, {'content': '   '}]
res = func(inputs)
print(res)
# [{'content': 'Valid content'}]
```
""")

add_chinese_doc('data.operators.filter_op.word_length_filter', """\
æŒ‰å•è¯å¹³å‡é•¿åº¦è¿‡æ»¤ï¼Œä¿ç•™åœ¨ [min_length, max_length) èŒƒå›´å†…çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    min_length (float): å•è¯å¹³å‡æœ€å°é•¿åº¦ï¼Œé»˜è®¤ 3
    max_length (float): å•è¯å¹³å‡æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ 20
""")

add_english_doc('data.operators.filter_op.word_length_filter', """\
Filter by average word length. Keeps text with mean word length in [min_length, max_length).

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    min_length (float): min avg word length, default 3
    max_length (float): max avg word length, default 20
""")

add_example('data.operators.filter_op.word_length_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.word_length_filter(input_key='content', min_length=3, max_length=10)
inputs = [{'content': 'I am ok'}, {'content': 'This is a normal sentence'}]
res = func(inputs)
print(res)
# [{'content': 'This is a normal sentence'}]
```
""")

add_chinese_doc('data.operators.filter_op.idcard_filter', """\
è¿‡æ»¤åŒ…å«è¿‡å¤šèº«ä»½è¯/è¯ä»¶ç›¸å…³è¯æ±‡çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    threshold (int): åŒ¹é…åˆ°ç›¸å…³è¯çš„æœ€å¤§æ•°é‡ï¼Œè¶…è¿‡åˆ™è¿‡æ»¤ï¼Œé»˜è®¤ 3
""")

add_english_doc('data.operators.filter_op.idcard_filter', """\
Filter text containing too many ID card / identity document related terms.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    threshold (int): max matches of related terms, filter if exceeded, default 3
""")

add_example('data.operators.filter_op.idcard_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.idcard_filter(input_key='content', threshold=1)
inputs = [{'content': 'è¿™æ˜¯æ­£å¸¸æ–‡æœ¬'}, {'content': 'è¯·æä¾›èº«ä»½è¯å·ç å’ŒID number'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯æ­£å¸¸æ–‡æœ¬'}]
```
""")

add_chinese_doc('data.operators.filter_op.no_punc_filter', """\
è¿‡æ»¤æ ‡ç‚¹ä¹‹é—´æ®µè·¯è¿‡é•¿çš„æ–‡æœ¬ï¼ˆå¦‚æ— æ ‡ç‚¹è¶…é•¿ä¸²ï¼‰ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_length_between_punct (int): æ ‡ç‚¹é—´æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ 112
    language (str): è¯­è¨€ï¼Œ'zh' æˆ– 'en'ï¼Œé»˜è®¤ 'zh'
""")

add_english_doc('data.operators.filter_op.no_punc_filter', """\
Filter text with too long segments between punctuation marks.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    max_length_between_punct (int): max length between punctuation, default 112
    language (str): language, 'zh' or 'en', default 'zh'
""")

add_example('data.operators.filter_op.no_punc_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.no_punc_filter(input_key='content', max_length_between_punct=20, language='zh')
inputs = [{'content': 'è¿™æ˜¯ã€‚æ­£å¸¸ã€‚æ–‡æœ¬ã€‚'}, {'content': 'è¿™æ˜¯ä¸€æ®µæ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„è¶…é•¿æ–‡æœ¬' * 10}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯ã€‚æ­£å¸¸ã€‚æ–‡æœ¬ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.special_char_filter', """\
è¿‡æ»¤åŒ…å«ç‰¹æ®Šä¸å¯è§å­—ç¬¦çš„æ–‡æœ¬ï¼ˆé›¶å®½å­—ç¬¦ã€æ›¿æ¢å­—ç¬¦ç­‰ï¼‰ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
""")

add_english_doc('data.operators.filter_op.special_char_filter', """\
Filter text containing special invisible characters (zero-width, replacement char, etc.).

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
""")

add_example('data.operators.filter_op.special_char_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.special_char_filter(input_key='content')
inputs = [{'content': 'Normal text æ­£å¸¸æ–‡æœ¬'}, {'content': 'Text with \u200b zero width'}]
res = func(inputs)
print(res)
# [{'content': 'Normal text æ­£å¸¸æ–‡æœ¬'}]
```
""")

add_chinese_doc('data.operators.filter_op.watermark_filter', """\
è¿‡æ»¤åŒ…å«ç‰ˆæƒ/æ°´å°ç›¸å…³è¯æ±‡çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    watermarks (list|None): è‡ªå®šä¹‰æ°´å°è¯åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨å†…ç½®åˆ—è¡¨
""")

add_english_doc('data.operators.filter_op.watermark_filter', """\
Filter text containing copyright/watermark related terms.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    watermarks (list|None): custom watermark terms, default uses built-in list
""")

add_example('data.operators.filter_op.watermark_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.watermark_filter(input_key='content')
inputs = [{'content': 'Normal content'}, {'content': 'This document contains Copyright notice'}]
res = func(inputs)
print(res)
# [{'content': 'Normal content'}]
```
""")

add_chinese_doc('data.operators.filter_op.curly_bracket_filter', """\
è¿‡æ»¤èŠ±æ‹¬å· {} å æ¯”è¿‡é«˜çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_ratio (float): èŠ±æ‹¬å·æœ€å¤§å æ¯”ï¼Œé»˜è®¤ 0.08
""")

add_english_doc('data.operators.filter_op.curly_bracket_filter', """\
Filter text with too high ratio of curly brackets {}.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    max_ratio (float): max ratio of curly brackets, default 0.08
""")

add_example('data.operators.filter_op.curly_bracket_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.curly_bracket_filter(input_key='content', max_ratio=0.08)
inputs = [{'content': 'Normal text'}, {'content': '{{{{{' * 10}]
res = func(inputs)
print(res)
# [{'content': 'Normal text'}]
```
""")

add_chinese_doc('data.operators.filter_op.lorem_ipsum_filter', """\
è¿‡æ»¤ Lorem ipsumã€å ä½ç¬¦ç­‰å ä½æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_ratio (float): å ä½æ¨¡å¼æœ€å¤§å‡ºç°æ¯”ä¾‹ï¼Œé»˜è®¤ 3e-8
""")

add_english_doc('data.operators.filter_op.lorem_ipsum_filter', """\
Filter Lorem ipsum, placeholder text, etc.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    max_ratio (float): max ratio of placeholder patterns, default 3e-8
""")

add_example('data.operators.filter_op.lorem_ipsum_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.lorem_ipsum_filter(input_key='content')
inputs = [{'content': 'This is real content'}, {'content': 'Lorem ipsum dolor sit amet'}]
res = func(inputs)
print(res)
# [{'content': 'This is real content'}]
```
""")

add_chinese_doc('data.operators.filter_op.unique_word_filter', """\
è¿‡æ»¤å»é‡åè¯æ•°å æ¯”è¿‡ä½çš„æ–‡æœ¬ï¼ˆé‡å¤è¯è¿‡å¤šçš„æ— æ•ˆå†…å®¹ï¼‰ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    min_ratio (float): å»é‡è¯æ•°æœ€å°å æ¯”ï¼Œé»˜è®¤ 0.1
    use_tokenizer (bool): æ˜¯å¦ä½¿ç”¨åˆ†è¯ï¼Œé»˜è®¤ True
    language (str): è¯­è¨€ï¼Œ'zh' æˆ– 'en'ï¼Œé»˜è®¤ 'zh'
""")

add_english_doc('data.operators.filter_op.unique_word_filter', """\
Filter text with too low unique word ratio (excessive repetition).

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    min_ratio (float): min unique word ratio, default 0.1
    use_tokenizer (bool): use tokenizer, default True
    language (str): language, 'zh' or 'en', default 'zh'
""")

add_example('data.operators.filter_op.unique_word_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.unique_word_filter(input_key='content', min_ratio=0.4, language='zh')
inputs = [{'content': 'è¿™æ˜¯ä¸€æ®µåŒ…å«å¤šä¸ªä¸åŒè¯æ±‡çš„æ–‡æœ¬ã€‚'}, {'content': 'é‡å¤é‡å¤é‡å¤'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯ä¸€æ®µåŒ…å«å¤šä¸ªä¸åŒè¯æ±‡çš„æ–‡æœ¬ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.char_count_filter', """\
æŒ‰å»é™¤ç©ºç™½åçš„å­—ç¬¦æ•°è¿‡æ»¤ï¼Œä¿ç•™åœ¨ [min_chars, max_chars] èŒƒå›´å†…çš„æ–‡æœ¬ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    min_chars (int): æœ€å°å­—ç¬¦æ•°ï¼Œé»˜è®¤ 100
    max_chars (int): æœ€å¤§å­—ç¬¦æ•°ï¼Œé»˜è®¤ 100000
""")

add_english_doc('data.operators.filter_op.char_count_filter', """\
Filter by character count (excluding whitespace). Keeps text in [min_chars, max_chars].

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    min_chars (int): min chars, default 100
    max_chars (int): max chars, default 100000
""")

add_example('data.operators.filter_op.char_count_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.char_count_filter(input_key='content', min_chars=10, max_chars=100)
inputs = [{'content': 'çŸ­'}, {'content': 'è¿™æ˜¯ä¸€æ®µä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬å†…å®¹ã€‚'}]
res = func(inputs)
print(res)
# [{'content': 'è¿™æ˜¯ä¸€æ®µä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬å†…å®¹ã€‚'}]
```
""")

add_chinese_doc('data.operators.filter_op.bullet_point_filter', """\
è¿‡æ»¤å­å¼¹ç‚¹è¡Œå æ¯”è¿‡é«˜çš„æ–‡æœ¬ï¼ˆå¦‚ç›®å½•ã€çº¯åˆ—è¡¨ï¼‰ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    max_ratio (float): ä»¥å­å¼¹ç‚¹å¼€å¤´çš„è¡Œæœ€å¤§å æ¯”ï¼Œé»˜è®¤ 0.9
""")

add_english_doc('data.operators.filter_op.bullet_point_filter', """\
Filter text with too many bullet-point lines (e.g. TOC, pure lists).

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    max_ratio (float): max ratio of bullet lines, default 0.9
""")

add_example('data.operators.filter_op.bullet_point_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.bullet_point_filter(input_key='content', max_ratio=0.5)
inputs = [{'content': 'Normal paragraph text'}, {'content': '- Item 1\\\\n- Item 2\\\\n- Item 3'}]
res = func(inputs)
print(res)
# [{'content': 'Normal paragraph text'}]
```
""")

add_chinese_doc('data.operators.filter_op.javascript_filter', """\
è¿‡æ»¤å«å¤§é‡ JavaScript ç›¸å…³æ¨¡å¼çš„æ–‡æœ¬ï¼ˆå¦‚ä»£ç ã€è„šæœ¬ç‰‡æ®µï¼‰ã€‚çŸ­æ–‡æœ¬(<=3è¡Œ)ä¸æ£€æµ‹ï¼Œç›´æ¥ä¿ç•™ï¼Œé¿å…è¯¯ä¼¤æ­£å¸¸çŸ­å¥ã€‚

Args:
    data (dict): å•æ¡æ•°æ®å­—å…¸
    input_key (str): æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'content'
    min_non_script_lines (int): æœ€å°‘éè„šæœ¬è¡Œæ•°ï¼Œé»˜è®¤ 3
""")

add_english_doc('data.operators.filter_op.javascript_filter', """\
Filter text containing many JavaScript patterns (code, script fragments). Short text (<=3 lines) is passed through to avoid false positives on normal short sentences.

Args:
    data (dict): single data dict
    input_key (str): key of the text field, default 'content'
    min_non_script_lines (int): min non-script lines, default 3
""")

add_example('data.operators.filter_op.javascript_filter', """\
```python
from lazyllm.tools.data import filter

func = filter.javascript_filter(input_key='content', min_non_script_lines=2)
inputs = [{'content': 'Short normal text'}, {'content': 'function() { return 1; }\nconst x = 1;\nvar y = 2;\nlet z = 3;'}]
res = func(inputs)
print(res)
# [{'content': 'Short normal text'}]
```
""")

# pipelines module docs
add_chinese_doc( 'data.pipelines.demo_pipelines.build_demo_pipeline', """\
æ„å»ºæ¼”ç¤ºç”¨æ•°æ®å¤„ç†æµæ°´çº¿ï¼ˆPipelineï¼‰ï¼ŒåŒ…å«è‹¥å¹²ç¤ºä¾‹ç®—å­å¹¶å±•ç¤ºå¦‚ä½•åœ¨ pipeline ä¸Šç»„åˆä½¿ç”¨è¿™äº›ç®—å­ã€‚

Args:
    input_key (str): è¦å¤„ç†çš„æ–‡æœ¬å­—æ®µåï¼Œé»˜è®¤ 'text'

**Returns:**\n
    ä¸€ä¸ªå¯è°ƒç”¨çš„ pipeline å¯¹è±¡ï¼Œè°ƒç”¨æ—¶ä¼šæŒ‰é¡ºåºæ‰§è¡Œå…¶ä¸­æ³¨å†Œçš„ç®—å­ã€‚
""")

add_english_doc('data.pipelines.demo_pipelines.build_demo_pipeline', """\
Build a demo data processing pipeline composed of several example operators.

Args:
    input_key (str): the text field name to process, default 'text'

**Returns:**\n
    A callable pipeline object that executes registered operators in sequence.
""")

add_example('data.pipelines.demo_pipelines.build_demo_pipeline', """\
```python
from lazyllm.tools.data.pipelines.demo_pipelines import build_demo_pipeline

ppl = build_demo_pipeline(input_key='text')
data = [{'text': 'lazyLLM'}]
res = ppl(data)
print(res)  # demonstrates how operators are combined and applied
```
""")
