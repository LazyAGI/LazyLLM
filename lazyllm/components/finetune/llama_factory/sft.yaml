### ModelArguments
model_name_or_path: internlm2-chat-7b
adapter_name_or_path: null
adapter_folder: null
cache_dir: null
use_fast_tokenizer: true
resize_vocab: false
split_special_tokens: false
new_special_tokens: null
model_revision: main
low_cpu_mem_usage: true
quantization_bit: null
quantization_type: nf4
double_quantization: true
quantization_device_map: null
rope_scaling: null
flash_attn: auto
shift_attn: false
mixture_of_depths: null
use_unsloth: false
# visual_inputs: false
moe_aux_loss_coef: null
disable_gradient_checkpointing: false
upcast_layernorm: false
upcast_lmhead_output: false
train_from_scratch: false
infer_backend: huggingface
vllm_maxlen: 2048
vllm_gpu_util: 0.9
vllm_enforce_eager: false
vllm_max_lora_rank: 32
offload_folder: offload
use_cache: true
infer_dtype: auto
hf_hub_token: null
ms_hub_token: null
export_dir: null
export_size: 1
export_device: cpu
export_quantization_bit: null
export_quantization_dataset: null
export_quantization_nsamples: 128
export_quantization_maxlen: 1024
export_legacy_format: false
export_hub_model_id: null
print_param_status: false

### DataArguments
template: null
dataset: identity,alpaca_en_demo
dataset_dir: lazyllm_temp_dir
# split: train
cutoff_len: 1024
train_on_prompt: false
streaming: false
buffer_size: 16384
mix_strategy: concat
interleave_probs: null
overwrite_cache: true
preprocessing_num_workers: 16
max_samples: 1000
eval_num_beams: null
ignore_pad_token_for_loss: true
val_size: 0.1
packing: null
tool_format: null
tokenized_path: null

### Seq2SeqTrainingArguments
output_dir: saves/sft
overwrite_output_dir: true
do_train: true
do_eval: false
do_predict: false
eval_strategy: steps
prediction_loss_only: false
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
per_gpu_train_batch_size: null
per_gpu_eval_batch_size: null
gradient_accumulation_steps: 1
eval_accumulation_steps: null
eval_delay: 0
learning_rate: 1.0e-04
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-08
max_grad_norm: 1.0
num_train_epochs: 3.0
max_steps: -1
lr_scheduler_type: cosine
lr_scheduler_kwargs: {}
warmup_ratio: 0.1
warmup_steps: 0
log_level: passive
log_level_replica: warning
log_on_each_node: true
logging_dir: null
logging_strategy: steps
logging_first_step: false
logging_steps: 10
logging_nan_inf_filter: true
save_strategy: steps
save_steps: 500
save_total_limit: null
save_safetensors: true
save_on_each_node: false
save_only_model: false
restore_callback_states_from_checkpoint: false
no_cuda: false
use_cpu: false
use_mps_device: false
seed: 42
data_seed: null
jit_mode_eval: false
use_ipex: false
bf16: false
fp16: true
fp16_opt_level: O1
half_precision_backend: auto
bf16_full_eval: false
fp16_full_eval: false
tf32: null
local_rank: -1
ddp_backend: null
tpu_num_cores: null
tpu_metrics_debug: false
debug: ''
dataloader_drop_last: false
eval_steps: 500
dataloader_num_workers: 0
dataloader_prefetch_factor: null
past_index: -1
run_name: null
disable_tqdm: null
remove_unused_columns: true
label_names: null
load_best_model_at_end: false
metric_for_best_model: null
greater_is_better: null
ignore_data_skip: false
fsdp: ''
fsdp_min_num_params: 0
fsdp_config: null
fsdp_transformer_layer_cls_to_wrap: null
accelerator_config: null
deepspeed: null
label_smoothing_factor: 0.0
optim: adamw_torch
optim_args: null
adafactor: false
group_by_length: false
length_column_name: length
report_to: tensorboard
ddp_find_unused_parameters: null
ddp_bucket_cap_mb: null
ddp_broadcast_buffers: null
dataloader_pin_memory: true
dataloader_persistent_workers: false
skip_memory_metrics: true
use_legacy_prediction_loop: false
push_to_hub: false
resume_from_checkpoint: null
hub_model_id: null
hub_strategy: every_save
hub_token: null
hub_private_repo: false
hub_always_push: false
gradient_checkpointing: false
gradient_checkpointing_kwargs: null
include_inputs_for_metrics: false
eval_do_concat_batches: true
fp16_backend: auto
evaluation_strategy: null
push_to_hub_model_id: null
push_to_hub_organization: null
push_to_hub_token: null
mp_parameters: ''
auto_find_batch_size: false
full_determinism: false
torchdynamo: null
ray_scope: last
ddp_timeout: 180000000
torch_compile: false
torch_compile_backend: null
torch_compile_mode: null
dispatch_batches: null
split_batches: null
include_tokens_per_second: false
include_num_input_tokens_seen: false
neftune_noise_alpha: null
optim_target_modules: null
batch_eval_metrics: false
sortish_sampler: false
predict_with_generate: false
generation_max_length: null
generation_num_beams: null
generation_config: null

### FinetuningArguments
use_badam: false
badam_mode: layer
badam_start_block: null
badam_switch_mode: ascending
badam_switch_interval: 50
badam_update_ratio: 0.05
badam_mask_mode: adjacent
badam_verbose: 0
use_galore: false
galore_target: all
galore_rank: 16
galore_update_interval: 200
galore_scale: 0.25
galore_proj_type: std
galore_layerwise: false
pref_beta: 0.1
pref_ftx: 0.0
pref_loss: sigmoid
dpo_label_smoothing: 0.0
kto_chosen_weight: 1.0
kto_rejected_weight: 1.0
simpo_gamma: 0.5
ppo_buffer_size: 1
ppo_epochs: 4
ppo_score_norm: false
ppo_target: 6.0
ppo_whiten_rewards: false
ref_model: null
ref_model_adapters: null
ref_model_quantization_bit: null
reward_model: null
reward_model_adapters: null
reward_model_quantization_bit: null
reward_model_type: lora
additional_target: null
lora_alpha: null
lora_dropout: 0.0
lora_rank: 8
lora_target: all
loraplus_lr_ratio: null
loraplus_lr_embedding: 1.0e-06
use_rslora: false
use_dora: false
pissa_init: false
pissa_iter: 4
pissa_convert: false
create_new_adapter: false
freeze_trainable_layers: 2
freeze_trainable_modules: all
freeze_extra_modules: null
pure_bf16: false
stage: sft
finetuning_type: lora
use_llama_pro: false
freeze_vision_tower: true
train_mm_proj_only: false
plot_loss: true

### GeneratingArguments
do_sample: true
temperature: 0.95
top_p: 0.7
top_k: 50
num_beams: 1
max_length: 1024
max_new_tokens: 1024
repetition_penalty: 1.0
length_penalty: 1.0
default_system: null