
# Building a streaming bot

Let's start by building a simple conversational bot that supports streaming.

> In this section, you will learn the following key points of LazyLLM:
> 
> - How to use [TrainableModule][lazyllm.module.TrainableModule] and [OnlineChatModule][lazyllm.module.onlineChatModule.OnlineChatModule] to build a conversational bot that supports streaming.
> - How to use a streaming chatbot with [FunctionCall][lazyllm.tools.agent.FunctionCall].

## Design Concept

Traditional models support streaming by setting the stream parameter to True during the request, and then the model's response will be returned in the form of a generator, that is, streaming output. However, in the [FunctionCall][lazyllm.tools.agent.FunctionCall] application, if streaming output is applied, the subsequent modules will not be able to determine in time whether the current request is [FunctionCall][lazyllm.tools.agent.FunctionCall] call. If you want to judge, you need to receive all the model outputs. Therefore, general [FunctionCall][lazyllm.tools.agent.FunctionCall] applications are non-streaming. Even if the parameter `stream=True` is set, the internal model response is received completely before subsequent processing, and it cannot really provide streaming output to users.

When faced with this problem, LazyLLM solves it through multi-threading. When the model is streamed, data is processed in two ways. One is to continuously push the messages generated by the model into the `FileSystemQueue` in a streaming manner until the special token related to the [FunctionCall][lazyllm.tools.agent.FunctionCall] call is detected, and then stop pushing the messages into the `FileSystemQueue`. The other is to receive messages normally until all the messages generated by the model are received, and then proceed to the subsequent message processing. When receiving messages, it is necessary to fetch data from the `FileSystemQueue` from the `FileSystemQueue` from another thread to obtain the messages generated by the model.

## Code Implementation

### Streaming conversational robot without front-end interface

Let's first simply implement a streaming conversational robot that only calls the model. The code is as follows:

```python
import lazyllm
from functools import partial

llm = lazyllm.TrainableModule("internlm2-chat-20b", stream=True) # or llm = lazyllm.OnlineChatModule(stream=True)

query = "What skills do you have?"

with lazyllm.ThreadPoolExecutor(1) as executor:
    future = executor.submit(partial(llm, llm_chat_history=[]), query)
	while True:
	    if value := lazyllm.FileSystemQueue().dequeue():
			print(f"output: {''.join(value)}")
		elif future.done():
			break
	print(f"ret: {future.result()}")
```

Here, you can use [TrainableModule][lazyllm.module.TrainableModule] or [OnlineChatModule][lazyllm.module.onlineChatModule.OnlineChatModule] because they have the same user experience. Here we take [TrainableModule][lazyllm.module.TrainableModule] as an example, and choose to use the `internlm2-chat-20b` model. When initializing the model, you need to specigy the parameter `stream=True`, and when receiveing messages, you need to use multi-threading to ensure that the messages output by the model can be received back in a streaming manner. Here you need to use the thread pool provided by LazyLLM to handle it, because it sets an identifier for the streaming response, so as to ensure that there is no confusion between the production thread and the consumer thread in a multi-threaded environment. If you use the thread pool provided by python to implement multi-threading, then you cannot get the message content from the queue correctly here.

### Streaming conversational robot with front-end interface

The main difference between this and a streaming chatbot without a front-end interface is that [WebModule][lazyllm.tools.webpages.WebModule] is responsible for streaming messages and displaying them on the front-ent interface. The code is as follows:

```python
import lazyllm

llm = lazyllm.TrainableModule("internlm2-chat-20b", stream=True)  # or llm = lazyllm.OnlineChatModule(stream=True)
lazyllm.WebModule(llm, port=23333).start().wait()
```

This is quite simple. You just need to define the model to use streaming, and then let [WebModule][lazyllm.tools.webpages.WebModule] handle the processing.

The effect is as follows:
![Stream_chat_bot](../assets/stream_cookbook_robot.png)

In fact, if you are [WebModule][lazyllm.tools.webpages.WebModule], you can also control whether to use streaming from the interface, that is, select or cancel the option of `流式输出` on the left side of the page. Isn't it simple?

OK, now that we have finished talking about the conversational robot, let's introduce how to use [FunctionCall][lazyllm.tools.agent.FunctionCall] in a streaming manner.

### Streaming chatbot with [FunctionCall][lazyllm.tools.agent.FunctionCall]

We first define the tools used by [FunctionCall][lazyllm.tools.agent.FunctionCall].

```python
import json
import lazyllm
from lazyllm import fc_register

@fc_register("tool")
def get_current_weather(location: str, unit: Literal["fahrenheit", "celsius"]='fahrenheit'):
    """
    Get the current weather in a given location

    Args:
        location (str): The city and state, e.g. San Francisco, CA.
        unit (str): The temperature unit to use. Infer this from the users location.
    """
    if 'tokyo' in location.lower():
        return json.dumps({'location': 'Tokyo', 'temperature': '10', 'unit': 'celsius'})
    elif 'san francisco' in location.lower():
        return json.dumps({'location': 'San Francisco', 'temperature': '72', 'unit': 'fahrenheit'})
    elif 'paris' in location.lower():
        return json.dumps({'location': 'Paris', 'temperature': '22', 'unit': 'celsius'})
    elif 'beijing' in location.lower():
        return json.dumps({'location': 'Beijing', 'temperature': '90', 'unit': 'Fahrenheit'})
    else:
        return json.dumps({'location': location, 'temperature': 'unknown'})

@fc_register("tool")
def get_n_day_weather_forecast(location: str, num_days: int, unit: Literal["celsius", "fahrenheit"]='fahrenheit'):
    """
    Get an N-day weather forecast

    Args:
        location (str): The city and state, e.g. San Francisco, CA.
        num_days (int): The number of days to forecast.
        unit (Literal['celsius', 'fahrenheit']): The temperature unit to use. Infer this from the users location.
    """
    if 'tokyo' in location.lower():
        return json.dumps({'location': 'Tokyo', 'temperature': '10', 'unit': 'celsius', "num_days": num_days})
    elif 'san francisco' in location.lower():
        return json.dumps({'location': 'San Francisco', 'temperature': '75', 'unit': 'fahrenheit', "num_days": num_days})
    elif 'paris' in location.lower():
        return json.dumps({'location': 'Paris', 'temperature': '25', 'unit': 'celsius', "num_days": num_days})
    elif 'beijing' in location.lower():
        return json.dumps({'location': 'Beijing', 'temperature': '85', 'unit': 'fahrenheit', "num_days": num_days})
    else:
        return json.dumps({'location': location, 'temperature': 'unknown'})
```

When defining a tool, you need to use the `fc_register` register and add comments to the tool so that LLM can distinguish whether is needs to be called and which tool to call. For specific precautions, see [ToolManager][lazyllm.tools.agent.ToolManager].

After defining the tools, we should define the model. Similarly, we can use [TrainableModule][lazyllm.module.TrainableModule] or [OnlineChatModule][lazyllm.module.onlineChatModule.OnlineChatModule]. Here we choose [TrainableModule][lazyllm.module.TrainableModule] and `internlm2-chat-20b` as the model.

```python
import lazyllm
llm = lazyllm.TrainableModule("internlm2-chat-20b", stream=True)  # or llm = lazyllm.OnlineChatModule()
```

It should be noted here that when using [TrainableModule][lazyllm.module.TrainableModule], you need to explicitly specify `stream=True`, because it defaults to `stream=False`. However, when using [OnlineChatModule][lazyllm.module.onlineChatModule.OnlineChatModule], you do not need to specify it, because [OnlineChatModule][lazyllm.module.onlineChatModule.OnlineChatModule] defaults to `stream=True`.

After defining the model, we should define [FunctionCallAgent][lazyllm.tools.agent.FunctionCallAgent]. [FunctionCallAgent][lazyllm.tools.agent.FunctionCallAgent] mainly passes in two parameters: model and toolset.

```python
from lazyllm.tools import FunctionCallAgent
tools = ["get_current_weather", "get_n_day_weather_forecast"]
agent = FunctionCallAgent(llm, tools)
```

Now there is only one last step left. We use [WebModule][lazyllm.tools.webpages.WebModule] to encapsulate the agent into a service with an interface.

```python
import lazyllm
lazyllm.WebModule(agent, port=23333).start().wait()
```

Now we have completed a conversational robot that supports streaming output and [FunctionCall][lazyllm.tools.agent.FunctionCall]. When there is information to show to the user, the interface will stream the message content. And [FunctionCall][lazyllm.tools.agent.FunctionCall] will execute normally.

The complete code is as follows:

```python
import json
import lazyllm
from lazyllm import fc_register, FunctionCallAgent

@fc_register("tool")
def get_current_weather(location: str, unit: Literal["fahrenheit", "celsius"]='fahrenheit'):
    """
    Get the current weather in a given location

    Args:
        location (str): The city and state, e.g. San Francisco, CA.
        unit (str): The temperature unit to use. Infer this from the users location.
    """
    if 'tokyo' in location.lower():
        return json.dumps({'location': 'Tokyo', 'temperature': '10', 'unit': 'celsius'})
    elif 'san francisco' in location.lower():
        return json.dumps({'location': 'San Francisco', 'temperature': '72', 'unit': 'fahrenheit'})
    elif 'paris' in location.lower():
        return json.dumps({'location': 'Paris', 'temperature': '22', 'unit': 'celsius'})
    elif 'beijing' in location.lower():
        return json.dumps({'location': 'Beijing', 'temperature': '90', 'unit': 'Fahrenheit'})
    else:
        return json.dumps({'location': location, 'temperature': 'unknown'})

@fc_register("tool")
def get_n_day_weather_forecast(location: str, num_days: int, unit: Literal["celsius", "fahrenheit"]='fahrenheit'):
    """
    Get an N-day weather forecast

    Args:
        location (str): The city and state, e.g. San Francisco, CA.
        num_days (int): The number of days to forecast.
        unit (Literal['celsius', 'fahrenheit']): The temperature unit to use. Infer this from the users location.
    """
    if 'tokyo' in location.lower():
        return json.dumps({'location': 'Tokyo', 'temperature': '10', 'unit': 'celsius', "num_days": num_days})
    elif 'san francisco' in location.lower():
        return json.dumps({'location': 'San Francisco', 'temperature': '75', 'unit': 'fahrenheit', "num_days": num_days})
    elif 'paris' in location.lower():
        return json.dumps({'location': 'Paris', 'temperature': '25', 'unit': 'celsius', "num_days": num_days})
    elif 'beijing' in location.lower():
        return json.dumps({'location': 'Beijing', 'temperature': '85', 'unit': 'fahrenheit', "num_days": num_days})
    else:
        return json.dumps({'location': location, 'temperature': 'unknown'})

llm = lazyllm.TrainableModule("internlm2-chat-20b", stream=True)  # or llm = lazyllm.OnlineChatModule()
tools = ["get_current_weather", "get_n_day_weather_forecast"]
agent = FunctionCallAgent(llm, tools)
lazyllm.WebModule(agent, port=23333).start().wait()
```

The effect is as follows:
![stream_agent](../assets/stream_cookbook_agent.png)

The interface will only display the content that the model needs to show to the user, and the tool call information generated by the model will not be printed out. Isn't it very simple? Similarly, other agents can also support streaming, which will not be shown here one by one.

This is the end of how to apply streaming in LazyyLLM. Later we can build our own applications according to our needs.
